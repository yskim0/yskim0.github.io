---
title: CH8. 가치 기반 에이전트
categories:
 - RL
tags: 
 - RL
---

[바닥부터 배우는 강화학습] `CH8. 가치 기반 에이전트`을 읽고 정리한 내용입니다.

---
전제
- 모델 프리
- 상태 공간과 액션 공간이 매우 큼

## RL 에이전트의 분류
- 가치 기반 agent : value-based
- 정책 기반 agent : policy-based 
- actor-critic : 가치 함수와 정책 함수 모두 사용
    - actor: policy, critic : v(s) or q(s,a)

# 8.1 밸류 네트워크의 학습
정책 pi가 고정되어 있을 때, 뉴럴넷을 이용하여 pi의 가치함수 $$ v_\pi (s) $$ 를 학습 => value network
- loss function
    - $$ L(\theta) = \mathbb{E}_\pi [(v_{true}(s) - v_\theta (s))^2] $$ 
        - 정책 함수 pi를 이용해 방문했던 상태 s에 대해  $$ (v_{true}(s) - v_\theta (s))^2 $$ 를 계산
    - $$ \delta_\theta L(\theta) = - \mathbb{E}_\pi [(v_{true}(s) - v_\theta (s))\delta_\theta v_\theta (s)] $$ 
- update : $$ \theta ' = \theta - \alpha \delta_\theta L(\theta) $$ 

## 8.2 Deep Q-Learning
- q(s,a)를 내재된 정책(implicit policy)로 사용함
- pseudo code보면 Q러닝이 off-policy임을 확인할 수 있음

## Experience Replay and Target Network
- DQN에 위 2가지 방법론을 도입함.
- Experience Replay
    - 겪었던 경험을 재사용하면 더 좋지 않을까라는 아이디어에서 출발
    - `replay buffer` : 가장 최근의 데이터 n개를 저장해 놓자는 아이디어
        - 데이터 효율성을 올림
    - off-policy 알고리즘에만 사용할 수 있는 개념임
- Target Network
    - Q러닝에서는 정답이 $$\theta$$에 의존적임.