---
title: CH9. 정책 기반 에이전트
categories:
 - RL
tags: 
 - RL
---

[바닥부터 배우는 강화학습] `CH9. 정책 기반 에이전트`을 읽고 정리한 내용입니다.

---
# Policy Gradient
- stochastic policy
- continuous action space

## 목적 함수 정하기
- what we want?
    - $$ \pi_\theta (s,a) $$ 에 대해 이 정책이 얼마나 좋은 정책인지 평가하는 방법을 찾고자 함
    - 평가함수 : $$ J(\theta) $$ 
        - \pi를 인풋으로 받아 점수를 리턴하는 함수
        - \pi가 \theta에 의해서 표현되니 \theta만 인풋으로 주어도 충분함
- $$ J(\theta) $$ 를 증가시키는 방향으로 gradient ascent를 하면 됨.
    - $$ \theta ' \leftarrow \theta + \alpha * \delta_\tetha J(\theta) $$

# Reinforce algorithm
- \pi_\theta 로 에피소드 하나에 해당하는 데이터를 얻고, 해당 데이터로 \theta 를 업데이트하고, 업데이트된 \pi_\theta 를 이용해 다음 에피소드의 경험을 얻고, 그 데이터로 강화하고, ... 반복
- policy graident 수식에서 Q자리에 리턴 G_t가 들어감
    - 편향되지 않은 샘플

# Actor-Critic
## Q 액터-크리틱
- \pi_\theta 는 실행할 a를 선택하는 액터 역할
- Q_W는 선택된 a의 벨류를 평가하는 크리틱 역할
- 에이전트 학습 과정에서 정책 pi와 밸류 Q를 모두 학습
- advantage A-C
    - 추가로 얼마의 가치를 더 얻게 되는가에 초점

## TD 액터-크리틱
- Advantage A-C는 3쌍의 뉴럴넷을 필요로 함 (pi, V, Q)
- TD 에러인 delta의 기댓값이 어드밴티지 A(s,a)가 됨. 즉 A(s,a)의 unbiased estimate
    - 같은 상태 s에서 같은 액션을 선택해도 상태 전이가 어떻게 일어나느냐에 따라서 매번 다른 값이 얻어지지만, 평균내면 그 값은 A(s,a)로 수렴함