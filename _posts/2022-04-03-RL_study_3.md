---
title: CH3. 벨만 방정식
categories:
 - RL
tags: 
 - RL
---

[바닥부터 배우는 강화학습] `CH3. 벨만 방정식`을 읽고 정리한 내용입니다.

---
주어진 정책의 상태적 value를 구하는 것은 생각보다 어려운 일이다. 
value를 계산하는 법을 한 줄로 "벨만 방정식"을 사용해서 구한다라고 얘기해도 큰 무리가 없을 정도로 중요한 수식이다.


벨만 방정식은 시점 t에서의 벨류와 시점 t+1에서의 벨류 사이의 관계를 다루며, 가치 함수와 정책 함수 사이의 관계도 다루고 있다.

## 벨만 기대 방정식
편의상 세 단계로 나눌 수 있다.
- 0단계


$$ v_\pi (s_t)=\mathbb{E}[r_{t+1} + \gamma v_\pi (s_{t+1})] $$


$$ q_\pi(s_t,a_t) = \mathbb{E}_\pi [r_{t+1}+\gamma q_\pi (s_{t+1}, a+{t+1})] $$

- 1단계


$$ v_\pi(s) = \sum_{a\in A} \pi(a|s) q_\pi (s,a) $$


$$ q_\pi(s,a) = r^a_s + \gamma \sum_{S' \in S} P^a_{SS'}v_\pi (S') $$

- 2단계


$$ v_\pi(s) = \sum_{a \in A} \pi(a|s) (r^a_s + \gamma \sum_{S' \in S} P^a_{SS'}v_\pi (S')) $$


$$ q_\pi(s,a) = r^a_s + \gamma \sum_{S' \in S} P^a_{SS'} \sum_{a' \in A} \pi (a'|s') q_\pi (s',a') $$


## 벨만 최적 방정식

### 최적 밸류와 최적 정책
- 최적의 정책 : $\pi_*$
- 최적의 밸류 : $ v_\ast (s) = v_{\pi_ \ast} (s,a) $  ($\pi_ \ast$ 를 따랐을 때의 밸류)
- 최적의 액션 밸류 : $ q_\ast (s,a) = q_{\pi_\ast} (s,a) $ ($\pi_\ast$ 를 따랐을 때의 액션 밸류)

### 벨만 최적 방정식

<img width="400" alt="image" src="https://user-images.githubusercontent.com/48315997/162716416-17346cb7-ea5e-47fb-a12f-82ddd8cb7c36.png">


