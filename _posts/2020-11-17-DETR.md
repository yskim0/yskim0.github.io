---
title: DETR:End-to-End Object Detection with Transformers
categories: [Paper Review, Vision]
tags: 
 - Deep Learning
 - Computer Vision
 - Object Detection
 - Encoder-Decoder
 - Transformer
 - Panoptic Segmentation
 - DETR
 - ViT

---


DETR : End-to-End Object Detection with Transformers 를 읽고 정리한 글입니다.

---


올해 가장 큰 인기를 몰고 있는 논문 중 하나인 DETR을 읽어보겠습니다.

Yann Lecun 교수님께서 삼성 AI 포럼에서 DETR 논문을 언급하면서 Transformer + Vision 의 연구가 이제 주 과제일 것이라고 말씀하셨습니다. 
DETR은 나오자마자 큰 화제가 되었기에 살짝 읽어보고 리뷰는 안하려 했으나, ViT 논문을 읽고 저도 이 분야의 장래가 유망하다고 인식하게 되어 이렇게 하나하나 뜯어봤습니다!

피드백은 언제나 환영합니다! 댓글창을 아직 열지 못해서 메일로 부탁드립니다. 꾸벅(__)

----


# Abstract

<br>

이 논문에서는 object detection을 하나의 **direct set prediction** 문제로 본다.

이 접근법은 **Non-maximum suppression, anchor generation**과 같은 작업들을 사용하지 않는 방법으로, detection pipeline을 매우 간소화시킬 수 있다.


논문에서 제안하는 모델은 DEtection Transformer(이하 DETR)이다. 이 모델의 주요 특징으로는 object detection을 set prediction problem으로 본다 하였으므로, **set 기반의 global loss를 사용**한다는 점이다. Transformer encoder-decoder 구조를 사용하여 bipartite matching을 통해 unique prediction을 한다.


학습된 object 쿼리들의 small set이 주어지면, DETR은 객체들과 전체 이미지 context 간의 관계에 대해 **병렬적(parallel)** 으로 final set에 대한 예측을 directly output으로 내보낸다.


이 모델은 library도 필요없고, 개념적으로 매우 심플하다.
COCO dataset에 대해서 Faster R-CNN baseline 급의 정확도와 런타임 성능을 보여줬다고 한다. 


게다가, DETR은 **panoptic segmentation**을 쉽게 일반화할 수 있다!


<br>


# Introduction

<br>

Object detection의 목표는 바운딩 박스 set과 label을 맞추는 것이다.


최근 대부분의 detector들은 이를 set of proposals, anchors, or window centers에 대해 regression/classification을 사용하는 '간접적인'방식으로 해결하려 한다.


하지만 이런 방식들은 postprocessing 단계에서 중복된 예측값들을 collapse 시키는 과정이나(NMS를 얘기하는 듯하다), 앵커 셋에 대해 미리 assign하는 휴리스틱한 방식 등에 의해 performance가 영향 받을 수 있다.


그래서 이 논문에서는 위와 같은 *Indirect* 방식이 아닌 ***Direct*** 방식을 사용한다. 


> We propose a direct set prediction approach to bypass the surrogate tasks.

이와 같은 end-to-end 방식은 기존의 object detection 연구에서는 자주 다뤄지지 않았던 것이다.


Obejct detection 문제를 direct set prediction problem으로 만들어 training pipeline을 만든다.
이 때, transformer 기반의 인코더-디코더 구조를 사용한다. 트랜스포머의 self-attention 기법은 elements들 간의 모든 pairwise interactions를 통해 중복된 prediction을 제거할 수 있게 해준다.


DETR은 모든 object들을 **한번에** predict한다. 이는 end-to-end 특성인데, 실제값과 예측값 사이에서 set loss를 통해 bipartite matching(이분 매칭)을 훈련한다. 이로 인해 DETR은 hand-designed 작업 없이 간단한 detection 파이프라인을 가질 수 있는 것이다.


다른 detection model들과 다르게 DETR은 customized layer도 필요하지 않아, 모델 자체도 굉장히 간단하다. CNN과 transformer만 가지고 있다면 쉽게 reproduce 할 수 있다.


이전의 direct set prediction 연구들과 비교할 때, DETR의 주요점은 bipartite matching(이분 매칭)과 parallel decoding을 결합시킨 것이다. 
DETR의 Matching loss function은 예측값을 ground truth에 uniquely 할당할 수 있고, predicted objects의 순열이 변하지 않기 때문에 병렬적으로 predict할 수 있다.


DETR은 COCO dataset에 evaluate 해봤을 때, Faster R-CNN baseline에 대하여 경쟁력 있는 결과를 보여주었다. 특히 큰 object들에 대하여 상당히 좋은 결과를 보여주었다고 한다. *하지만, small object들에 대해서는 low performance를 보였다.* 이는 아마 Faster R-CNN에 FPN을 적용하여 좋은 성과를 이뤘듯이 future work에서도 해결법이 생기지 않을까 기대한다.


DETR은 복잡한 문제를 쉽게 확장시킬 수 있다. 실험에서는 op pretrained DETR을 이용하여 segmentation head를 학습시켰더니 Panoptic Segmentation에 경쟁력을 가졌다고 한다.


<br>

# Related Work


<br>


> Our work build on prior work in several domains: bipartite matching losses for
set prediction, encoder-decoder architectures based on the transformer, parallel
decoding, and object detection methods.


<br>

# The DETR model

<br>

set prediction을 위해 가장 중요한 요소는 2개이다.

1. **a set prediction loss** that forces **unique matching** between predicted and ground truth boxes
2. **an architecture** that **predicts** (in a single pass) a set of objects and models their relation


## Object detection set prediction loss

DETR은 fixed-size set에서 N개의 prediction을 한다. **이 때 N은 이미지의 object 개수보다 상당히 크게 설정한다.**


ground truth에 대하여 predicted objects(class, position, size)를 scoring하는 것이 학습과정에서 어려웠다고 한다.
여기서 loss는 예측값과 ground truth 사이의 최적의 이분 매칭을 만들어 내고, 이를 통해 bounding box loss를 최적화한다.


$y$를 ground truth, $\hat{y} = {\hat{y_i}}_{i=1}^N$ 은 N개의 prediction set이라 하자.


N은 이미지내의 object 개수보다 크다고 가정하면, $y$도 a set of size N에 $\varnothing$ (no object)가 padded 됐다고 볼 수 있다.


이러한 y, y_hat 사이의 bipartite matching(이는 ground truth와 prediction 사이의 일대일 매칭이라 생각하면 편할 듯) 찾기 위해서, matching cost가 minimum이 되는 바운딩 박스의 순서들, 즉 순열을 찾아야 한다.


이러한 optimal assignent는 헝가리안 알고리즘에 의해 효율적으로 계산될 수 있음.


이제 다시 matching cost에 대해 알아보자. each element of *i* of the ground truth set은 $y_i = (c_i, b_i)$ 로 나타낼 수 있음. 이 때 c_i는 target class label, b_i는 box center coordinates(xy의 center, height, width) 임. 


$$L_{Hungarian}(y,\hat{y} = \mathbb{-1}_{c_i\neq\varnothing} + \mathbb{1}L_{box}(b_i, \hat{b_{\sigma(i)}})$$


앞의 항에 -1을 곱하는 이유는 해당 클래스 probability가 높을수록 loss를 낮게 산정하기 위함이고, 뒷 항은 bounding box의 로스를 크로스 엔트로피와 같은 과정없이 그대로 적용한다는 것이 조금 특이한 지점이다.


여기서 이 논문의 차별점 혹은 중요한 지점은 중복(duplicates)없이 direct set prediction을 위한 일대일 매칭이 된다는 점이다. 즉, 중복되는 prediction이 존재하지 않다는 뜻. **(일대일 매칭을 하기 때문에 중복되는 예측이 생기지 않음 -> NMS가 필요없다!!)**


***다른 모델에서는 앵커박스를 많이 뽑아서 하나 물체에 여러 개의 바운딩 박스를 그려서 NMS(Non-maximum suppression)을 사용하는 방식이었음!!***



이렇게 일대일 매칭된 prediction과 ground truth 사이에서는 위에서 언급했듯이 헝가리안 기반의 loss를 사용해서 학습함. (즉 방금했던 것은 matching cost를 계산하는 loss였고, 이번이 진짜 예측값과 Ground Truth사이의 loss인듯?) 


![image](https://user-images.githubusercontent.com/48315997/99262847-c8c19280-2861-11eb-8c4a-ea37362777ae.png)



L_match와 약간 비슷해보이지만, class 확률 값을 no-object 일 때도 계산한다는 점, log를 사용해 cross entropy같은 성질이 추가되었다는 점이 다르다.
클래스가 no_object일 때 앞의 항의 loss를 1/10으로 줄여 class imbalance를 조절했다고 한다.



마지막으로 bounding box loss를 살펴보자. 다른 detector들은 미리 정해놓은 앵커 박스 candidate들을 얼마나 움직일지에 대한 **델타**를 학습했다. 그러나 DETR에서는 바운딩 박스를 directly 예측한다. L1 Loss를 사용하는데, L1 loss의 단점은 바운딩 박스가 크면 작은 거에 비해 loss가 더 큼. (단순히 좌표의차이만을 고려하니까)


이 단점을 보완하기 위해 Generalized IoU loss를 L1 Loss에 linear combination 했다.


![image](https://user-images.githubusercontent.com/48315997/99262815-c0695780-2861-11eb-8170-8b6271d35ddd.png)



## DETR Architecture

![image](https://user-images.githubusercontent.com/48315997/99263078-11794b80-2862-11eb-9452-ca8b498d812c.png)

[요약]
- CNN으로 feature map 생성
- 트랜스포머 인코더에 들어가기 위해서 positional encoding 더함
- 트랜스포머 인코더의 출력은 디코더에 들어감
    - 이 값들이 **key, value** 역할을 함.
- 디코더의 input으로는 N개의 object **query**가 들어간다.
    - 이 쿼리도 학습하게 됨. 처음에는 랜덤값으로 채우지만, 점점 학습하는 것
    - 일종의 positional encoding 역할을 함 (어느 부분을 관심있게 볼 것인지)
- 각각 FFN을 통해서 N개를 예측하게 된다.



<br>


DETR의 전체적인 구조는 위 Figure과 같다. 크게 3가지 컴포넌트로 구성된다. 


1. CNN backbone : CNN을 이용해 compact feature representation을 추출한다.(feature map)

2. Encoder-Decoder Transformer 

3. A simple Feed Forward Network(FFN) : 결과값 출력


DETR의 구조는 매우 간단해서 베이스라인 모델을 파이토치로 구현하면 50줄정도이다! (글 하단부에 첨부하겠음.)


### Backbone

실험에서는 ResNet을 사용함. H, W는 5번 정도의 down-scale을 했다.(H0/32, W0/32)
최종 C = 2048로 사용(ResNet 모델의 마지막 레이어가 2048이기 때문)


<img width="609" alt="스크린샷 2020-11-17 오전 12 10 58" src="https://user-images.githubusercontent.com/48315997/99269155-61a7dc00-2869-11eb-9ee9-9d13ef280c23.png">



### Transformer encoder

우선 1x1 conv.를 통해 채널을 $d$ dimension으로 줄인다.


또 인코더에 들어가기 위해선 벡터로 바뀌어야 하니까 3차원을 2차원으로 바꿔야 한다. 
H,W를 다 합쳐서 spatial dimensions of d x HW로 만들어준다.
이 때 HW는 시퀀스의 개수가 되고, 각 시퀀스 벡터의 사이즈가 d가 되는 것이다.


트랜스포머는 attention기반이기 때문에 permutation-invariant하다. 즉, 순서를 무시한다는 것이다.


때문에, 순서를 부여하기 위해 fixed positional encoding을 input of each attention layer에 추가한다. 
*(참고) x, y축 따로 d/2씩 해서 인코딩해서 concat하는 식으로 positional encoding을 해준다.*



### Transformer decoder

transformer의 스탠다드 아키텍쳐를 따랐다고 한다.

다만 original과의 차이점은 각 디코더 레이어에서 N개의 객체를 **parallel하게 디코딩**한다는 것이다.


디코더를 통과하게 되면 N개의 오브젝트가 parallel하게 나오는데, 이 때 디코더의입력으로 들어가는 것은 object queries이다. 


디코더 역시 **permutation-invariant** 하기 때문에 일종의 positional encoding이 필요하다. (입력으로 들어가는 것들이 다 달라야 다른 결과를 뽑아줄 수 있기 때문이다)


여기서 object query가 positional encoding 역할을 한다.
이 쿼리는 학습의 대상이 되며 each attention layer의 인풋으로 추가된다.


이것들은 **independently FFN을 지나** 박스 좌표, 클래스 라벨로 디코드된다. (그렇게 N개의 final prediction을 얻게 된다.)


개인적으로 가장 중요한 부분이라고 생각하는게, 이러한 self- and encoder-decoder attention 모델은 image를 **global 추론할 수 있게 해준다.** 즉 use the whole image as context로 사용할 수 있는 것이다!!




### Prediction feed-forward networks (FFNs)


3-layer 퍼셉트론 with ReLU 구조이다. 


이 FFN은 bounding box의 normalized center coordinates, height, width를 예측하며, softmax function을 통해 class label을 예측한다.


no object 클래스는 background role을 하기도 한다.

<br>


# Experiments

<br>

다는 살펴보지 않고, 몇 개만 살펴볼 것임.


## Comparison with Faster R-CNN

![image](https://user-images.githubusercontent.com/48315997/99273109-eb58a900-286b-11eb-9c47-c2bbeae303b1.png)


AP_S : Small scale images

AP_L : Large scale images


CNN은 local 특성을 잘 반영하므로 small img.들에 DETR보다 더 효과적이다.

하지만 DETR은 Attention 기반이기 때문에 모든 position을 볼 수 있고, 때문에 Large img.에 매우 효과적이다.

*그러나 최근 나온 논문 Deformable DETR에서 AP_S도 Faster R-CNN보다 높은 성과를 거두었다! (다음 논문은 이걸 읽어보려고 한다.)*

*(Attention이 global한 reason이 된다는 장점이 있기 때문에 panoptic segmentation에 잘 맞지 않을까하는 생각이 든다...)*

## Number of encoder layers

<img width="644" alt="스크린샷 2020-11-17 오전 12 33 56" src="https://user-images.githubusercontent.com/48315997/99273597-95383580-286c-11eb-9501-87a86806a63f.png">


인코더의 블럭수가 많을수록 AP는 올라간다. 


또한 **Encoder는 disentangling**(이미지 분해, 해체) 역할에 아주중요한 역할을 한다는 것을 알 수 있었다고 한다.


![image](https://user-images.githubusercontent.com/48315997/99273849-e3e5cf80-286c-11eb-8f92-c330bddc9e51.png)


image disentanglement 란 위의 사진과 같이 객체별로(인스탄스별로) 잘 분리가 됐음을 표현하는 듯하다.

이렇게 객체별로 attention이 잘 나누어진다면 디코더에서 객체의 bounding box, class를 예측하는 것은 매우 쉽다고 한다.


## Number of decoder layers


![image](https://user-images.githubusercontent.com/48315997/99274139-4dfe7480-286d-11eb-9d6d-e06ab83853f3.png)


이것은 decoder layer의 개수가 어느정도 되면, NMS를 쓸 때와 성능상 별 차이가 없다는 결과이다.


![image](https://user-images.githubusercontent.com/48315997/99274340-9322a680-286d-11eb-811d-ae3aeb96712f.png)


개인적으로 정말 놀랐던 결과이다. 객체의 head, edge를 정말 잘 attention하고 있음을 알 수 있다. 
**인코더는 global attention을 통해 인스턴스들을 분리한다면, 디코더는 클래스와 객체의 바운더리(가장자리)를 추출한다고 한다**.


## Panoptic Segmentation

![image](https://user-images.githubusercontent.com/48315997/99275057-60c57900-286e-11eb-87fa-4c29646eb60c.png)


<br>

# Conclusion


<br>

DETR은 object detection 문제를 direct set prediction으로 보았고, 이를 Transformer와 bipartite matching을 통한 end-to-end 방식으로 풀어냈다. (많은 연구자들이 모든 딥러닝 모델의 최종 지향점은 end-to-end라고 한다.)


transformer 특성상, DETR 또한 architecture를 flexible 하게 확장시킬 수 있다.(Panoptic segmentation처럼) 




----


# Code

![image](https://user-images.githubusercontent.com/48315997/99277842-ab94c000-2871-11eb-803f-d34e1bc3406b.png)


<br>


# Additional studies
(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)

Deformable DETR

<br>

# References
(References for your additional studies)

- [인간지능이 인공지능을 공부하는 장소](https://keyog.tistory.com/32)
- [KP's blog](https://kp1994.tistory.com/15)
- [TF 논문 읽기 모임 PR-284](https://www.youtube.com/watch?v=lXpBcW_I54U&feature=youtu.be&fbclid=IwAR25dYNnKxCsN5apneKrmPus-umovk7fziMedDQMqBfhg28eaBj6u-tRxzI&ab_channel=JinWonLee)